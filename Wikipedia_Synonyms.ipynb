{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell\n",
    "import pywikibot\n",
    "import wikipedia\n",
    "from wikipedia import WikipediaPage\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Note in Advance\n",
    "# Using wikipedia for finding synonyms and paraphrases of Named Entities would\n",
    "# include using the redirections for each page for perfect results\n",
    "# This can be added\n",
    "# The keyword \"Myanmar\" will not return \"Burma\" ATM as Burma is a redirection page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be the keyword search\n",
    "keyword = \"Elizabeth II\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = wikipedia.search(keyword)\n",
    "\n",
    "result = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = WikipediaPage(keyword)\n",
    "keyword_url = page.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def parse(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvlimit\": 1,\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"Me\"}\n",
    "    req = requests.get(API_URL, headers=headers, params=params)\n",
    "    res = req.json()\n",
    "    revision = res[\"query\"][\"pages\"][0][\"revisions\"][0]\n",
    "    text = revision[\"slots\"][\"main\"][\"content\"]\n",
    "    return mwparserfromhell.parse(text)\n",
    "\n",
    "\n",
    "links = parse(keyword).filter_wikilinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elizabeth II': '/wiki/Elizabeth_II',\n",
       " 'Queen Elizabeth II': '/wiki/Elizabeth_II',\n",
       " 'Elizabeth\\xa0II': '/wiki/Elizabeth_II',\n",
       " 'her': '/wiki/Elizabeth_II',\n",
       " 'Elizabeth': '/wiki/Elizabeth_II',\n",
       " 'Princess Elizabeth': '/wiki/Elizabeth_II',\n",
       " 'The Queen': '/wiki/Elizabeth_II',\n",
       " 'Elizabeth II of the United Kingdom': '/wiki/Elizabeth_II',\n",
       " 'the Queen': '/wiki/Elizabeth_II',\n",
       " 'Princess (later Queen) Elizabeth': '/wiki/Elizabeth_II',\n",
       " 'Queen': '/wiki/Elizabeth_II',\n",
       " 'Queen Elizabeth II of the United Kingdom': '/wiki/Elizabeth_II',\n",
       " 'Lilibet': '/wiki/Elizabeth_II',\n",
       " 'Princess\\xa0Elizabeth, Duchess\\xa0of\\xa0Edinburgh': '/wiki/Elizabeth_II',\n",
       " 'Princess Elizabeth, Duchess of Edinburgh': '/wiki/Elizabeth_II',\n",
       " 'Her Majesty the Queen': '/wiki/Elizabeth_II',\n",
       " 'Queen Elizabeth': '/wiki/Elizabeth_II',\n",
       " 'monarch': '/wiki/Elizabeth_II',\n",
       " 'HM Queen Elizabeth II': '/wiki/Elizabeth_II',\n",
       " 'HM The Queen': '/wiki/Elizabeth_II',\n",
       " 'Princess Elizabeth of the United Kingdom': '/wiki/Elizabeth_II',\n",
       " 'QueenElizabeth II': '/wiki/Elizabeth_II',\n",
       " 'Elizabeth II (until 1966)': '/wiki/Elizabeth_II'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# For Elisabeth, there are >700 articles in \"links\"\n",
    "# Using not the whole list of links would also be an option\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        # preprocess the link\n",
    "        wiki = WikipediaPage(str(link)[2:(len(str(link))-2)])\n",
    "        keyword_url = wiki.url\n",
    "        res = requests.get(keyword_url)\n",
    "        soup = bs(res.text, \"html.parser\")\n",
    "        for soup in soup.find_all(\"a\"):\n",
    "            url = soup.get(\"href\", \"\")\n",
    "            # Idea for stating the conditon later: Find wiki link to keyword\n",
    "            if url.endswith(\"wiki/\"+keyword.replace(\" \", \"_\")):\n",
    "                results[soup.text.strip()] = url\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional step:\n",
    "# now delete any word from the list that is not a named entity (like \"her\"/ \"monarch\")\n",
    "# Also some basic preprocessing (for \"Princess\\xa0Elizabeth\" etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
